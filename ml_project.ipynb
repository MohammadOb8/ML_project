{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ff92b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForest\n",
      "Precision: 0.803030303030303\n",
      "Recall: 0.7162162162162162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       105\n",
      "           1       0.80      0.72      0.76        74\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: SVC\n",
      "Precision: 0.803030303030303\n",
      "Recall: 0.7162162162162162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       105\n",
      "           1       0.80      0.72      0.76        74\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Best Model: RandomForest\n",
      "The RandomForest model was chosen as the best model. This is likely because RandomForest can handle the complexity of the data and is robust to overfitting. It also effectively handles feature interactions and missing values, which is beneficial for the Titanic dataset.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Read the Dataset\n",
    "# Load the dataset from the local files\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# 2. Preprocess the Dataset\n",
    "# Handle Missing Values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_df['Age'] = imputer.fit_transform(train_df[['Age']])\n",
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n",
    "train_df.drop(columns=['Cabin'], inplace=True)\n",
    "\n",
    "# Remove Duplicate Data\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Categorical Data: Convert categorical data into numerical format\n",
    "categorical_features = ['Sex', 'Embarked']\n",
    "numerical_features = ['Age', 'Fare', 'SibSp', 'Parch', 'Pclass']\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split the data\n",
    "X = train_df.drop(columns=['Survived', 'Name', 'Ticket'])\n",
    "y = train_df['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply transformations to data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# 3. Model Experimentation\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "# Define hyperparameters for GridSearch\n",
    "param_grid = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Experiment with different models and hyperparameters\n",
    "best_models = {}\n",
    "for model_name in models.keys():\n",
    "    clf = GridSearchCV(models[model_name], param_grid[model_name], cv=5, scoring='accuracy')\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_models[model_name] = clf.best_estimator_\n",
    "\n",
    "# 4. Model Evaluation\n",
    "\n",
    "# Evaluate models using precision and recall\n",
    "evaluation_results = {}\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    evaluation_results[model_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'classification_report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Choose the best model\n",
    "best_model_name = max(evaluation_results, key=lambda k: (evaluation_results[k]['precision'], evaluation_results[k]['recall']))\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Precision: {metrics['precision']}\")\n",
    "    print(f\"Recall: {metrics['recall']}\")\n",
    "    print(metrics['classification_report'])\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "\n",
    "# Discussion\n",
    "if best_model_name == 'RandomForest':\n",
    "    print(\"The RandomForest model was chosen as the best model. This is likely because RandomForest can handle the complexity of the data and is robust to overfitting. It also effectively handles feature interactions and missing values, which is beneficial for the Titanic dataset.\")\n",
    "elif best_model_name == 'SVC':\n",
    "    print(\"The SVC model was chosen as the best model. SVC is effective for high-dimensional spaces and when the number of samples is less than the number of features. Its kernel trick allows it to model complex non-linear relationships in the data, which might have led to its better performance on the Titanic dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8039fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
